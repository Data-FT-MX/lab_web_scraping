{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "import random\n",
    "import re\n",
    "import html5lib\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'\n",
    "response= requests.get(url)\n",
    "contenido= response.content\n",
    "soup= BeautifulSoup(contenido,'lxml')\n",
    "nombre= soup.select('h1.h3.lh-condensed a')\n",
    "nombre[0]['href']\n",
    "lista_nombre= [i ['href'] for i in nombre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/me-no-dev',\n",
       " '/emilk',\n",
       " '/MichaIng',\n",
       " '/twpayne',\n",
       " '/dai-shi',\n",
       " '/hawkw',\n",
       " '/chenjiahan',\n",
       " '/davidpdrsn',\n",
       " '/PySimpleGUI',\n",
       " '/benbjohnson',\n",
       " '/shivammathur',\n",
       " '/unknwon',\n",
       " '/seanmonstar',\n",
       " '/tmm',\n",
       " '/akinsho',\n",
       " '/Feichtmeier',\n",
       " '/sethmlarson',\n",
       " '/Rich-Harris',\n",
       " '/LinuxSuRen',\n",
       " '/tpope',\n",
       " '/Ralim',\n",
       " '/rappasoft',\n",
       " '/zkochan',\n",
       " '/ckerr',\n",
       " '/rusty1s']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_nombre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Z4nzu/hackingtool',\n",
       " '/NVlabs/nvdiffrec',\n",
       " '/OpenBB-finance/OpenBBTerminal',\n",
       " '/github/copilot-docs',\n",
       " '/chaosec2021/Spring-cloud-function-SpEL-RCE',\n",
       " '/theOehrly/Fast-F1',\n",
       " '/MIC-DKFZ/nnUNet',\n",
       " '/facebookresearch/detectron2',\n",
       " '/commaai/openpilot',\n",
       " '/microsoft/recommenders',\n",
       " '/vinta/awesome-python',\n",
       " '/psf/black',\n",
       " '/zulip/zulip',\n",
       " '/huggingface/course',\n",
       " '/cjhutto/vaderSentiment',\n",
       " '/donnemartin/system-design-primer',\n",
       " '/deepchecks/deepchecks',\n",
       " '/streamlink/streamlink',\n",
       " '/aws/aws-cli',\n",
       " '/frappe/frappe',\n",
       " '/open-mmlab/mmocr',\n",
       " '/dagster-io/dagster',\n",
       " '/liuhuanyong/QASystemOnMedicalKG',\n",
       " '/ashutosh1919/explainable-cnn',\n",
       " '/d2l-ai/d2l-en']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "response= requests.get(url)\n",
    "contenido= response.content\n",
    "soup= BeautifulSoup(contenido,'lxml')\n",
    "\n",
    "repo= soup.select('h1.h3.lh-condensed a')\n",
    "repo[0]['href']\n",
    "lista_repo= [i ['href'] for i in repo]\n",
    "lista_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/19px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/15px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/19px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/21px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/21px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/14px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/16px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/21px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "response= requests.get(url)\n",
    "contenido= response.content\n",
    "soup= BeautifulSoup(contenido,'lxml')\n",
    "\n",
    "#image= soup.select('img')\n",
    "#lista_img= [i['src']for i in image]\n",
    "#print(len(lista_img))\n",
    "image2= soup.select('div.mw-parser-output img')\n",
    "lista_img_2= [i['src']for i in image2]\n",
    "lista_img_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '#Computing',\n",
       " '#People',\n",
       " '#Roller_coasters',\n",
       " '#Vehicles',\n",
       " '#Weaponry',\n",
       " '#Other_uses',\n",
       " '#See_also',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/wiki/Colt_Python',\n",
       " '/wiki/PYTHON',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/wiki/Timon_of_Phlius',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/Pithon']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "response= requests.get(url)\n",
    "contenido= response.content\n",
    "soup= BeautifulSoup(contenido,'lxml')\n",
    "\n",
    "#soup = BeautifulSoup(requests.get(url).content, 'lxml')\n",
    "list_urls = soup.select('div.mw-parser-output li a')\n",
    "urls = [ref['href'] for ref in list_urls]\n",
    "urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 9 - Arbitration ٭',\n",
       " 'Title 16 - Conservation',\n",
       " 'Title 18 - Crimes and Criminal Procedure ٭',\n",
       " 'Title 20 - Education',\n",
       " 'Title 21 - Food and Drugs',\n",
       " 'Title 31 - Money and Finance ٭',\n",
       " \"Title 38 - Veterans' Benefits ٭\",\n",
       " 'Title 41 - Public Contracts ٭',\n",
       " 'Title 42 - The Public Health and Welfare',\n",
       " 'Title 48 - Territories and Insular Possessions']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "response= requests.get(url)\n",
    "contenido= response.content\n",
    "soup= BeautifulSoup(contenido,'lxml')\n",
    "\n",
    "list_cambios = soup.select('div.usctitlechanged')\n",
    "cambios= [name.text.strip() for name in list_cambios]\n",
    "cambios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'EUGENE PALMER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'OCTAVIANO JUAREZ-CORRO']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "response= requests.get(url)\n",
    "contenido= response.content\n",
    "soup= BeautifulSoup(contenido,'lxml')\n",
    "\n",
    "list_fbi = soup.select('li h3')\n",
    "fbi=[name.text.strip() for name in list_fbi]\n",
    "fbi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html= requests.get(url).content\n",
    "table= pd.read_html(html)\n",
    "table_df=table[3]\n",
    "table_df=table_df.drop(columns=[(    'CitizenResponse',   '12345678910»'),\n",
    "                      (    'CitizenResponse', '12345678910».1'),\n",
    "                      (    'CitizenResponse', '12345678910».2'),\n",
    "                      (           'Depth km',   '12345678910»'),\n",
    "                      (           'Mag [+]',   '12345678910»'),\n",
    "                      (    'Last update [-]',   '12345678910»'),\n",
    "                      ('Unnamed: 12_level_0',   '12345678910»')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Date &amp; Time UTC</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Latitude degrees</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Longitude degrees</th>\n",
       "      <th>Region name [+]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>12345678910»</th>\n",
       "      <th>12345678910»</th>\n",
       "      <th>12345678910».1</th>\n",
       "      <th>12345678910»</th>\n",
       "      <th>12345678910».1</th>\n",
       "      <th>12345678910»</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-03 17:29:30.012min ago</td>\n",
       "      <td>4.42</td>\n",
       "      <td>S</td>\n",
       "      <td>77.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-03 17:25:44.016min ago</td>\n",
       "      <td>9.23</td>\n",
       "      <td>N</td>\n",
       "      <td>126.89</td>\n",
       "      <td>E</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-03 17:07:29.234min ago</td>\n",
       "      <td>43.11</td>\n",
       "      <td>N</td>\n",
       "      <td>9.10</td>\n",
       "      <td>W</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-03 17:01:01.640min ago</td>\n",
       "      <td>42.84</td>\n",
       "      <td>N</td>\n",
       "      <td>12.86</td>\n",
       "      <td>E</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-03 16:58:16.543min ago</td>\n",
       "      <td>38.19</td>\n",
       "      <td>N</td>\n",
       "      <td>117.82</td>\n",
       "      <td>W</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-04-03 16:45:33.156min ago</td>\n",
       "      <td>33.48</td>\n",
       "      <td>N</td>\n",
       "      <td>116.57</td>\n",
       "      <td>W</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-04-03 16:41:52.81hr 00min ago</td>\n",
       "      <td>40.65</td>\n",
       "      <td>N</td>\n",
       "      <td>29.10</td>\n",
       "      <td>E</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-04-03 16:39:14.01hr 02min ago</td>\n",
       "      <td>4.72</td>\n",
       "      <td>S</td>\n",
       "      <td>139.56</td>\n",
       "      <td>E</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-04-03 16:30:03.11hr 11min ago</td>\n",
       "      <td>49.45</td>\n",
       "      <td>N</td>\n",
       "      <td>144.43</td>\n",
       "      <td>E</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-04-03 16:15:25.01hr 26min ago</td>\n",
       "      <td>9.17</td>\n",
       "      <td>N</td>\n",
       "      <td>126.76</td>\n",
       "      <td>E</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-04-03 16:15:05.01hr 26min ago</td>\n",
       "      <td>9.68</td>\n",
       "      <td>S</td>\n",
       "      <td>121.92</td>\n",
       "      <td>E</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-04-03 15:46:51.41hr 55min ago</td>\n",
       "      <td>26.88</td>\n",
       "      <td>N</td>\n",
       "      <td>126.49</td>\n",
       "      <td>E</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-04-03 15:45:25.21hr 56min ago</td>\n",
       "      <td>35.88</td>\n",
       "      <td>N</td>\n",
       "      <td>3.73</td>\n",
       "      <td>W</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-04-03 15:29:43.12hr 12min ago</td>\n",
       "      <td>9.16</td>\n",
       "      <td>N</td>\n",
       "      <td>126.55</td>\n",
       "      <td>E</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-04-03 15:24:45.02hr 17min ago</td>\n",
       "      <td>21.14</td>\n",
       "      <td>S</td>\n",
       "      <td>68.58</td>\n",
       "      <td>W</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-04-03 15:16:38.42hr 25min ago</td>\n",
       "      <td>41.85</td>\n",
       "      <td>N</td>\n",
       "      <td>20.22</td>\n",
       "      <td>E</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-04-03 15:00:03.52hr 41min ago</td>\n",
       "      <td>36.01</td>\n",
       "      <td>N</td>\n",
       "      <td>69.82</td>\n",
       "      <td>E</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-04-03 14:52:53.02hr 49min ago</td>\n",
       "      <td>0.11</td>\n",
       "      <td>N</td>\n",
       "      <td>126.67</td>\n",
       "      <td>E</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-04-03 14:46:40.42hr 55min ago</td>\n",
       "      <td>52.92</td>\n",
       "      <td>N</td>\n",
       "      <td>157.19</td>\n",
       "      <td>E</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-04-03 14:32:46.03hr 09min ago</td>\n",
       "      <td>40.01</td>\n",
       "      <td>S</td>\n",
       "      <td>74.73</td>\n",
       "      <td>W</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-04-03 14:18:46.03hr 23min ago</td>\n",
       "      <td>8.12</td>\n",
       "      <td>S</td>\n",
       "      <td>123.86</td>\n",
       "      <td>E</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-04-03 13:51:17.03hr 50min ago</td>\n",
       "      <td>22.32</td>\n",
       "      <td>S</td>\n",
       "      <td>67.81</td>\n",
       "      <td>W</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-04-03 13:45:42.03hr 56min ago</td>\n",
       "      <td>11.45</td>\n",
       "      <td>N</td>\n",
       "      <td>126.00</td>\n",
       "      <td>E</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022-04-03 13:30:02.14hr 11min ago</td>\n",
       "      <td>0.75</td>\n",
       "      <td>N</td>\n",
       "      <td>99.88</td>\n",
       "      <td>E</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022-04-03 13:21:16.04hr 20min ago</td>\n",
       "      <td>29.95</td>\n",
       "      <td>N</td>\n",
       "      <td>80.33</td>\n",
       "      <td>E</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2022-04-03 13:14:13.54hr 27min ago</td>\n",
       "      <td>32.30</td>\n",
       "      <td>S</td>\n",
       "      <td>178.94</td>\n",
       "      <td>E</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022-04-03 12:51:07.94hr 50min ago</td>\n",
       "      <td>18.02</td>\n",
       "      <td>N</td>\n",
       "      <td>66.77</td>\n",
       "      <td>W</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2022-04-03 12:49:09.04hr 52min ago</td>\n",
       "      <td>6.34</td>\n",
       "      <td>S</td>\n",
       "      <td>103.91</td>\n",
       "      <td>E</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2022-04-03 12:38:21.25hr 03min ago</td>\n",
       "      <td>19.15</td>\n",
       "      <td>N</td>\n",
       "      <td>155.37</td>\n",
       "      <td>W</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2022-04-03 12:37:21.25hr 04min ago</td>\n",
       "      <td>37.74</td>\n",
       "      <td>N</td>\n",
       "      <td>25.85</td>\n",
       "      <td>E</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2022-04-03 12:27:32.05hr 14min ago</td>\n",
       "      <td>23.27</td>\n",
       "      <td>N</td>\n",
       "      <td>88.99</td>\n",
       "      <td>E</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2022-04-03 12:23:57.05hr 17min ago</td>\n",
       "      <td>9.15</td>\n",
       "      <td>N</td>\n",
       "      <td>126.70</td>\n",
       "      <td>E</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2022-04-03 12:22:00.55hr 19min ago</td>\n",
       "      <td>64.29</td>\n",
       "      <td>N</td>\n",
       "      <td>149.87</td>\n",
       "      <td>W</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2022-04-03 12:11:56.65hr 29min ago</td>\n",
       "      <td>40.08</td>\n",
       "      <td>N</td>\n",
       "      <td>22.33</td>\n",
       "      <td>E</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2022-04-03 12:07:06.75hr 34min ago</td>\n",
       "      <td>17.95</td>\n",
       "      <td>N</td>\n",
       "      <td>66.88</td>\n",
       "      <td>W</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022-04-03 12:06:27.05hr 35min ago</td>\n",
       "      <td>47.58</td>\n",
       "      <td>N</td>\n",
       "      <td>9.05</td>\n",
       "      <td>E</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2022-04-03 12:01:00.75hr 40min ago</td>\n",
       "      <td>34.05</td>\n",
       "      <td>N</td>\n",
       "      <td>58.78</td>\n",
       "      <td>E</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2022-04-03 11:56:54.85hr 45min ago</td>\n",
       "      <td>19.22</td>\n",
       "      <td>N</td>\n",
       "      <td>155.40</td>\n",
       "      <td>W</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2022-04-03 11:53:31.85hr 48min ago</td>\n",
       "      <td>37.27</td>\n",
       "      <td>N</td>\n",
       "      <td>118.43</td>\n",
       "      <td>W</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2022-04-03 11:53:12.05hr 48min ago</td>\n",
       "      <td>24.05</td>\n",
       "      <td>S</td>\n",
       "      <td>67.26</td>\n",
       "      <td>W</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2022-04-03 11:52:32.05hr 49min ago</td>\n",
       "      <td>35.68</td>\n",
       "      <td>N</td>\n",
       "      <td>3.52</td>\n",
       "      <td>W</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2022-04-03 11:39:18.66hr 02min ago</td>\n",
       "      <td>37.28</td>\n",
       "      <td>N</td>\n",
       "      <td>21.13</td>\n",
       "      <td>E</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022-04-03 11:35:19.46hr 06min ago</td>\n",
       "      <td>42.09</td>\n",
       "      <td>N</td>\n",
       "      <td>8.12</td>\n",
       "      <td>W</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2022-04-03 11:35:19.36hr 06min ago</td>\n",
       "      <td>9.28</td>\n",
       "      <td>N</td>\n",
       "      <td>126.90</td>\n",
       "      <td>E</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2022-04-03 11:22:07.36hr 19min ago</td>\n",
       "      <td>9.25</td>\n",
       "      <td>N</td>\n",
       "      <td>126.63</td>\n",
       "      <td>E</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2022-04-03 10:38:57.97hr 02min ago</td>\n",
       "      <td>32.29</td>\n",
       "      <td>S</td>\n",
       "      <td>138.52</td>\n",
       "      <td>E</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2022-04-03 10:37:00.07hr 04min ago</td>\n",
       "      <td>11.51</td>\n",
       "      <td>N</td>\n",
       "      <td>126.06</td>\n",
       "      <td>E</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2022-04-03 10:34:41.67hr 07min ago</td>\n",
       "      <td>17.99</td>\n",
       "      <td>N</td>\n",
       "      <td>66.97</td>\n",
       "      <td>W</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2022-04-03 10:24:57.57hr 16min ago</td>\n",
       "      <td>9.23</td>\n",
       "      <td>N</td>\n",
       "      <td>126.65</td>\n",
       "      <td>E</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2022-04-03 10:17:20.07hr 24min ago</td>\n",
       "      <td>11.46</td>\n",
       "      <td>N</td>\n",
       "      <td>126.04</td>\n",
       "      <td>E</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>12345678910»</td>\n",
       "      <td>12345678910»</td>\n",
       "      <td>12345678910»</td>\n",
       "      <td>12345678910»</td>\n",
       "      <td>12345678910»</td>\n",
       "      <td>12345678910»</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date & Time UTC Latitude degrees                  \\\n",
       "                         12345678910»    12345678910» 12345678910».1   \n",
       "0       2022-04-03 17:29:30.012min ago             4.42               S   \n",
       "1       2022-04-03 17:25:44.016min ago             9.23               N   \n",
       "2       2022-04-03 17:07:29.234min ago            43.11               N   \n",
       "3       2022-04-03 17:01:01.640min ago            42.84               N   \n",
       "4       2022-04-03 16:58:16.543min ago            38.19               N   \n",
       "5       2022-04-03 16:45:33.156min ago            33.48               N   \n",
       "6   2022-04-03 16:41:52.81hr 00min ago            40.65               N   \n",
       "7   2022-04-03 16:39:14.01hr 02min ago             4.72               S   \n",
       "8   2022-04-03 16:30:03.11hr 11min ago            49.45               N   \n",
       "9   2022-04-03 16:15:25.01hr 26min ago             9.17               N   \n",
       "10  2022-04-03 16:15:05.01hr 26min ago             9.68               S   \n",
       "11  2022-04-03 15:46:51.41hr 55min ago            26.88               N   \n",
       "12  2022-04-03 15:45:25.21hr 56min ago            35.88               N   \n",
       "13  2022-04-03 15:29:43.12hr 12min ago             9.16               N   \n",
       "14  2022-04-03 15:24:45.02hr 17min ago            21.14               S   \n",
       "15  2022-04-03 15:16:38.42hr 25min ago            41.85               N   \n",
       "16  2022-04-03 15:00:03.52hr 41min ago            36.01               N   \n",
       "17  2022-04-03 14:52:53.02hr 49min ago             0.11               N   \n",
       "18  2022-04-03 14:46:40.42hr 55min ago            52.92               N   \n",
       "19  2022-04-03 14:32:46.03hr 09min ago            40.01               S   \n",
       "20  2022-04-03 14:18:46.03hr 23min ago             8.12               S   \n",
       "21  2022-04-03 13:51:17.03hr 50min ago            22.32               S   \n",
       "22  2022-04-03 13:45:42.03hr 56min ago            11.45               N   \n",
       "23  2022-04-03 13:30:02.14hr 11min ago             0.75               N   \n",
       "24  2022-04-03 13:21:16.04hr 20min ago            29.95               N   \n",
       "25  2022-04-03 13:14:13.54hr 27min ago            32.30               S   \n",
       "26  2022-04-03 12:51:07.94hr 50min ago            18.02               N   \n",
       "27  2022-04-03 12:49:09.04hr 52min ago             6.34               S   \n",
       "28  2022-04-03 12:38:21.25hr 03min ago            19.15               N   \n",
       "29  2022-04-03 12:37:21.25hr 04min ago            37.74               N   \n",
       "30  2022-04-03 12:27:32.05hr 14min ago            23.27               N   \n",
       "31  2022-04-03 12:23:57.05hr 17min ago             9.15               N   \n",
       "32  2022-04-03 12:22:00.55hr 19min ago            64.29               N   \n",
       "33  2022-04-03 12:11:56.65hr 29min ago            40.08               N   \n",
       "34  2022-04-03 12:07:06.75hr 34min ago            17.95               N   \n",
       "35  2022-04-03 12:06:27.05hr 35min ago            47.58               N   \n",
       "36  2022-04-03 12:01:00.75hr 40min ago            34.05               N   \n",
       "37  2022-04-03 11:56:54.85hr 45min ago            19.22               N   \n",
       "38  2022-04-03 11:53:31.85hr 48min ago            37.27               N   \n",
       "39  2022-04-03 11:53:12.05hr 48min ago            24.05               S   \n",
       "40  2022-04-03 11:52:32.05hr 49min ago            35.68               N   \n",
       "41  2022-04-03 11:39:18.66hr 02min ago            37.28               N   \n",
       "42  2022-04-03 11:35:19.46hr 06min ago            42.09               N   \n",
       "43  2022-04-03 11:35:19.36hr 06min ago             9.28               N   \n",
       "44  2022-04-03 11:22:07.36hr 19min ago             9.25               N   \n",
       "45  2022-04-03 10:38:57.97hr 02min ago            32.29               S   \n",
       "46  2022-04-03 10:37:00.07hr 04min ago            11.51               N   \n",
       "47  2022-04-03 10:34:41.67hr 07min ago            17.99               N   \n",
       "48  2022-04-03 10:24:57.57hr 16min ago             9.23               N   \n",
       "49  2022-04-03 10:17:20.07hr 24min ago            11.46               N   \n",
       "50                                 NaN              NaN             NaN   \n",
       "51                       12345678910»    12345678910»   12345678910»   \n",
       "52                                 NaN              NaN             NaN   \n",
       "\n",
       "   Longitude degrees                 Region name [+]  \n",
       "       12345678910» 12345678910».1   12345678910»  \n",
       "0              77.00               W             4.7  \n",
       "1             126.89               E             3.1  \n",
       "2               9.10               W             2.1  \n",
       "3              12.86               E             2.0  \n",
       "4             117.82               W             2.7  \n",
       "5             116.57               W             2.4  \n",
       "6              29.10               E             2.0  \n",
       "7             139.56               E             4.1  \n",
       "8             144.43               E             4.1  \n",
       "9             126.76               E             3.6  \n",
       "10            121.92               E             2.9  \n",
       "11            126.49               E             4.6  \n",
       "12              3.73               W             2.1  \n",
       "13            126.55               E             5.6  \n",
       "14             68.58               W             2.5  \n",
       "15             20.22               E             2.5  \n",
       "16             69.82               E             4.2  \n",
       "17            126.67               E             4.2  \n",
       "18            157.19               E             4.0  \n",
       "19             74.73               W             3.5  \n",
       "20            123.86               E             3.5  \n",
       "21             67.81               W             2.8  \n",
       "22            126.00               E             3.7  \n",
       "23             99.88               E             4.4  \n",
       "24             80.33               E             3.0  \n",
       "25            178.94               E             3.9  \n",
       "26             66.77               W             2.3  \n",
       "27            103.91               E             4.1  \n",
       "28            155.37               W             2.2  \n",
       "29             25.85               E             2.5  \n",
       "30             88.99               E             3.9  \n",
       "31            126.70               E             3.5  \n",
       "32            149.87               W             3.8  \n",
       "33             22.33               E             2.3  \n",
       "34             66.88               W             2.0  \n",
       "35              9.05               E             1.3  \n",
       "36             58.78               E             3.9  \n",
       "37            155.40               W             2.7  \n",
       "38            118.43               W             2.2  \n",
       "39             67.26               W             3.5  \n",
       "40              3.52               W             2.0  \n",
       "41             21.13               E             3.1  \n",
       "42              8.12               W             1.8  \n",
       "43            126.90               E             4.7  \n",
       "44            126.63               E             5.4  \n",
       "45            138.52               E             3.2  \n",
       "46            126.06               E             3.7  \n",
       "47             66.97               W             2.1  \n",
       "48            126.65               E             5.5  \n",
       "49            126.04               E             4.0  \n",
       "50               NaN             NaN             NaN  \n",
       "51     12345678910»   12345678910»   12345678910»  \n",
       "52               NaN             NaN             NaN  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No se encontró ninguna página"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Related articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>b'6 458 000+'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Русский</td>\n",
       "      <td>b'1 798 000+'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>日本語</td>\n",
       "      <td>b'1 314 000+'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Español</td>\n",
       "      <td>b'1 755 000+'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>b'2 667 000+'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Français</td>\n",
       "      <td>b'2 400 000+'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>中文</td>\n",
       "      <td>b'1 256 000+'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>b'1 742 000+'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Português</td>\n",
       "      <td>b'1 085 000+'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Polski</td>\n",
       "      <td>b'1 512 000+'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Language Related articles\n",
       "0    English    b'6 458 000+'\n",
       "1    Русский    b'1 798 000+'\n",
       "2        日本語    b'1 314 000+'\n",
       "3    Español    b'1 755 000+'\n",
       "4    Deutsch    b'2 667 000+'\n",
       "5   Français    b'2 400 000+'\n",
       "6         中文    b'1 256 000+'\n",
       "7   Italiano    b'1 742 000+'\n",
       "8  Português    b'1 085 000+'\n",
       "9     Polski    b'1 512 000+'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "response= requests.get(url)\n",
    "contenido= response.content\n",
    "soup= BeautifulSoup(contenido,'lxml')\n",
    "\n",
    "list_language= soup.select('div a strong')\n",
    "#list_language[0].string\n",
    "language= [i.text for i in list_language]\n",
    "\n",
    "rel_art= soup.select('div a small bdi')\n",
    "num=[i.text for i in rel_art]\n",
    "list_num=[i.replace(u'\\xa0', ' ').encode('utf-8') for i in num]\n",
    "list_num\n",
    "\n",
    "wiki= pd.DataFrame(zip(language,list_num))\n",
    "new_col=['Language', 'Related articles']\n",
    "wiki.columns=new_col\n",
    "wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "response= requests.get(url)\n",
    "contenido= response.content\n",
    "soup= BeautifulSoup(contenido,'lxml')\n",
    "\n",
    "links= soup.select('li h3 a')\n",
    "listalin= [i['href'] for i in links]\n",
    "listalin= [i.replace('/search?filters%5Btopic%5D=','') for i in listalin]\n",
    "listalin= [i.replace('+', ' ') for i in listalin]\n",
    "listalin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>Percentageof world pop.(March 2019)[10]</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "      <td>11.922%</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "      <td>5.994%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "      <td>4.922%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hindi (sanskritised Hindustani)[11]</td>\n",
       "      <td>341</td>\n",
       "      <td>4.429%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>300</td>\n",
       "      <td>4.000%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "      <td>2.870%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "      <td>2.000%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "      <td>1.662%</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Western Punjabi[12]</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.079%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Telugu</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.065%</td>\n",
       "      <td>Dravidian</td>\n",
       "      <td>South-Central</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                             Language Speakers(millions)  \\\n",
       "0      1                     Mandarin Chinese                918   \n",
       "1      2                              Spanish                480   \n",
       "2      3                              English                379   \n",
       "3      4  Hindi (sanskritised Hindustani)[11]                341   \n",
       "4      5                              Bengali                300   \n",
       "5      6                           Portuguese                221   \n",
       "6      7                              Russian                154   \n",
       "7      8                             Japanese                128   \n",
       "8      9                  Western Punjabi[12]               92.7   \n",
       "9     10                              Marathi               83.1   \n",
       "10    11                               Telugu               82.0   \n",
       "\n",
       "   Percentageof world pop.(March 2019)[10] Language family         Branch  \n",
       "0                                  11.922%    Sino-Tibetan        Sinitic  \n",
       "1                                   5.994%   Indo-European        Romance  \n",
       "2                                   4.922%   Indo-European       Germanic  \n",
       "3                                   4.429%   Indo-European     Indo-Aryan  \n",
       "4                                   4.000%   Indo-European     Indo-Aryan  \n",
       "5                                   2.870%   Indo-European        Romance  \n",
       "6                                   2.000%   Indo-European   Balto-Slavic  \n",
       "7                                   1.662%         Japonic       Japanese  \n",
       "8                                   1.204%   Indo-European     Indo-Aryan  \n",
       "9                                   1.079%   Indo-European     Indo-Aryan  \n",
       "10                                  1.065%       Dravidian  South-Central  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "html= requests.get(url).content\n",
    "table= pd.read_html(html)\n",
    "table[1][0:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank &amp; Title</th>\n",
       "      <th>IMDb Rating</th>\n",
       "      <th>Director and Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.  Sueño de fuga  (1994)</td>\n",
       "      <td>9.2</td>\n",
       "      <td>Frank Darabont (dir.), Tim Robbins, Morgan Fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.  El padrino  (1972)</td>\n",
       "      <td>9.2</td>\n",
       "      <td>Francis Ford Coppola (dir.), Marlon Brando, Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.  Batman: El Caballero de la Noche  (2008)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Christopher Nolan (dir.), Christian Bale, Heat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.  El padrino II  (1974)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Francis Ford Coppola (dir.), Al Pacino, Robert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.  12 hombres en pugna  (1957)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Rank & Title  IMDb Rating  \\\n",
       "0                     1.  Sueño de fuga  (1994)          9.2   \n",
       "1                        2.  El padrino  (1972)          9.2   \n",
       "2  3.  Batman: El Caballero de la Noche  (2008)          9.0   \n",
       "3                     4.  El padrino II  (1974)          9.0   \n",
       "4               5.  12 hombres en pugna  (1957)          9.0   \n",
       "\n",
       "                                  Director and Stars  \n",
       "0  Frank Darabont (dir.), Tim Robbins, Morgan Fre...  \n",
       "1  Francis Ford Coppola (dir.), Marlon Brando, Al...  \n",
       "2  Christopher Nolan (dir.), Christian Bale, Heat...  \n",
       "3  Francis Ford Coppola (dir.), Al Pacino, Robert...  \n",
       "4      Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb  "
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "html= requests.get(url).content\n",
    "table= pd.read_html(html)\n",
    "table= table[0]\n",
    "table.columns\n",
    "table= table.drop(columns=['Unnamed: 0', 'Your Rating', 'Unnamed: 4'])\n",
    "\n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "response= requests.get(url)\n",
    "contenido= response.content\n",
    "soup= BeautifulSoup(contenido,'lxml')\n",
    "\n",
    "direk= soup.select('td a')\n",
    "a=direk[1:500:2]\n",
    "b= [i['title'] for i in a]\n",
    "\n",
    "table['Director and Stars']= b\n",
    "table.head()\n",
    "#b=re.findall(r'\\w.+?\\)', a) <-- para hallar al director y artistas\n",
    "#re.findall(r'\\w.+?\\)', b[0]) <-- prueba separando al director de los artistas\n",
    "#c=[re.findall(r'\\w.+?\\)', i) for i in b] <-- separando al director de los artistas en bucle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramte\\AppData\\Local\\Temp\\ipykernel_22100\\4067679518.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  IMDB['Reseñas']= lista_reseñas\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank &amp; Title</th>\n",
       "      <th>Reseñas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.  Sueño de fuga  (1994)</td>\n",
       "      <td>Two imprisoned men bond over a number of years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.  El padrino  (1972)</td>\n",
       "      <td>The aging patriarch of an organized crime dyna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.  Batman: El Caballero de la Noche  (2008)</td>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.  El padrino II  (1974)</td>\n",
       "      <td>The early life and career of Vito Corleone in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.  12 hombres en pugna  (1957)</td>\n",
       "      <td>The jury in a New York City murder trial is fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.  La lista de Schindler  (1993)</td>\n",
       "      <td>In German-occupied Poland during World War II,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.  El señor de los anillos: El retorno del re...</td>\n",
       "      <td>Gandalf and Aragorn lead the World of Men agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.  Tiempos violentos  (1994)</td>\n",
       "      <td>The lives of two mob hitmen, a boxer, a gangst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.  El señor de los anillos: La comunidad del ...</td>\n",
       "      <td>A meek Hobbit from the Shire and eight compani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.  El bueno, el malo y el feo  (1966)</td>\n",
       "      <td>A bounty hunting scam joins two men in an unea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.  Forrest Gump  (1994)</td>\n",
       "      <td>The presidencies of Kennedy and Johnson, the V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Rank & Title  \\\n",
       "0                           1.  Sueño de fuga  (1994)   \n",
       "1                              2.  El padrino  (1972)   \n",
       "2        3.  Batman: El Caballero de la Noche  (2008)   \n",
       "3                           4.  El padrino II  (1974)   \n",
       "4                     5.  12 hombres en pugna  (1957)   \n",
       "5                   6.  La lista de Schindler  (1993)   \n",
       "6   7.  El señor de los anillos: El retorno del re...   \n",
       "7                       8.  Tiempos violentos  (1994)   \n",
       "8   9.  El señor de los anillos: La comunidad del ...   \n",
       "9             10.  El bueno, el malo y el feo  (1966)   \n",
       "10                          11.  Forrest Gump  (1994)   \n",
       "\n",
       "                                              Reseñas  \n",
       "0   Two imprisoned men bond over a number of years...  \n",
       "1   The aging patriarch of an organized crime dyna...  \n",
       "2   When the menace known as the Joker wreaks havo...  \n",
       "3   The early life and career of Vito Corleone in ...  \n",
       "4   The jury in a New York City murder trial is fr...  \n",
       "5   In German-occupied Poland during World War II,...  \n",
       "6   Gandalf and Aragorn lead the World of Men agai...  \n",
       "7   The lives of two mob hitmen, a boxer, a gangst...  \n",
       "8   A meek Hobbit from the Shire and eight compani...  \n",
       "9   A bounty hunting scam joins two men in an unea...  \n",
       "10  The presidencies of Kennedy and Johnson, the V...  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generacion de tabla primaria\n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "html= requests.get(url).content\n",
    "table= pd.read_html(html)\n",
    "table= table[0]\n",
    "table= table.drop(columns=['Unnamed: 0', 'Your Rating', 'Unnamed: 4','IMDb Rating'])\n",
    "IMDB= table[0:11]\n",
    "\n",
    "#Obtencion de las urls\n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "response= requests.get(url)\n",
    "contenido= response.content\n",
    "soup= BeautifulSoup(contenido,'lxml')\n",
    "\n",
    "a= soup.select('td a')\n",
    "b=a[1:500:2]\n",
    "c=[i['href'] for i in b]\n",
    "lista_urls=c[0:11]\n",
    "new_url='https://www.imdb.com'\n",
    "lis=[new_url+i for i in lista_urls]\n",
    "lis\n",
    "\n",
    "# Obtencion de los resumenes\n",
    "lista_reseñas=[]\n",
    "for i in lis[0:11]:\n",
    "    response= requests.get(i)\n",
    "    contenido= response.content\n",
    "    soup= BeautifulSoup(contenido,'lxml')\n",
    "    reseña= soup.select('p span')\n",
    "    reseña=reseña[0].text\n",
    "    lista_reseñas.append(reseña)\n",
    "    \n",
    "#Aditamenteo de la lista_reseñas a la tabla proncipal\n",
    "IMDB['Reseñas']= lista_reseñas\n",
    "IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city:mexico\n"
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock (22 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets of Getting Your Dream...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A Novel Based on the Life of...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the Boat: Nine Americans and Their...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade Trilogy, #1)</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little Life (Scott Pi...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and Start Again</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be Your Life: Scenes from the A...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science Fiction Stories 18...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title   Price  \\\n",
       "0                                A Light in the Attic  £51.77   \n",
       "1                                  Tipping the Velvet  £53.74   \n",
       "2                                          Soumission  £50.10   \n",
       "3                                       Sharp Objects  £47.82   \n",
       "4               Sapiens: A Brief History of Humankind  £54.23   \n",
       "5                                     The Requiem Red  £22.65   \n",
       "6   The Dirty Little Secrets of Getting Your Dream...  £33.34   \n",
       "7   The Coming Woman: A Novel Based on the Life of...  £17.93   \n",
       "8   The Boys in the Boat: Nine Americans and Their...  £22.60   \n",
       "9                                     The Black Maria  £52.15   \n",
       "10     Starving Hearts (Triangular Trade Trilogy, #1)  £13.99   \n",
       "11                              Shakespeare's Sonnets  £20.66   \n",
       "12                                        Set Me Free  £17.46   \n",
       "13  Scott Pilgrim's Precious Little Life (Scott Pi...  £52.29   \n",
       "14                          Rip it Up and Start Again  £35.02   \n",
       "15  Our Band Could Be Your Life: Scenes from the A...  £57.25   \n",
       "16                                               Olio  £23.88   \n",
       "17  Mesaerion: The Best Science Fiction Stories 18...  £37.59   \n",
       "18                       Libertarianism for Beginners  £51.33   \n",
       "19                            It's Only the Himalayas  £45.17   \n",
       "\n",
       "                      Stock  \n",
       "0   In stock (22 available)  \n",
       "1   In stock (20 available)  \n",
       "2   In stock (20 available)  \n",
       "3   In stock (20 available)  \n",
       "4   In stock (20 available)  \n",
       "5   In stock (19 available)  \n",
       "6   In stock (19 available)  \n",
       "7   In stock (19 available)  \n",
       "8   In stock (19 available)  \n",
       "9   In stock (19 available)  \n",
       "10  In stock (19 available)  \n",
       "11  In stock (19 available)  \n",
       "12  In stock (19 available)  \n",
       "13  In stock (19 available)  \n",
       "14  In stock (19 available)  \n",
       "15  In stock (19 available)  \n",
       "16  In stock (19 available)  \n",
       "17  In stock (19 available)  \n",
       "18  In stock (19 available)  \n",
       "19  In stock (19 available)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://books.toscrape.com/'\n",
    "soup = BeautifulSoup(requests.get(url).content, 'lxml')\n",
    "\n",
    "a= soup.select('h3 a')\n",
    "titles= [i['title'] for i in a]\n",
    "#titles\n",
    "\n",
    "b=soup.select('div p.price_color')\n",
    "prices= [i.text for i in b]\n",
    "#prices\n",
    "\n",
    "#Obtencion de la extension para obtener stock\n",
    "ta= soup.select('h3 a')\n",
    "te= [i['href'] for i in a]\n",
    "\n",
    "#obtencion de urls\n",
    "lisli=[]\n",
    "[lisli.append(url+i) for i in te]\n",
    "   \n",
    "\n",
    "#obtencion de datos por medio de la informacion de la tabla\n",
    "listoc=[]\n",
    "for i in lisli:\n",
    "    html= requests.get(i).content\n",
    "    table= pd.read_html(html)\n",
    "    z=table[0]\n",
    "    y=z.iloc[5]\n",
    "    listoc.append(y[1])\n",
    "\n",
    "\n",
    "#Construccion main Df\n",
    "book= pd.DataFrame(zip(titles, prices,listoc))\n",
    "bocol= ['Title','Price','Stock']\n",
    "book.columns= bocol\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stock (22 available)\n"
     ]
    }
   ],
   "source": [
    "url = 'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "html= requests.get(url).content\n",
    "table= pd.read_html(html)\n",
    "z=table[0]\n",
    "y=z.iloc[5]\n",
    "print(y[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
